{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "pat = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The client uses OpenAI Library version 0.27.0. To use 1.0.0, uncomment the line below\n",
    "client = openai.OpenAI(api_key=pat)\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.1, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load unit tests\n",
    "unittest_name = \"addition_test\"\n",
    "\n",
    "with open(f\"./{unittest_name}.py\", \"r\") as file:\n",
    "    unittest_code = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial prompt\n",
    "\n",
    "lang = \"Python\"\n",
    "\n",
    "\n",
    "initial_prompt = prompt = f\"\"\"\n",
    "\n",
    "You're a {lang} software developer. You will write code to make the unit tests given in the triple backticks passed. You will use the strategies of test driven development to do this. Provide the code to make the first test pass.\n",
    "\n",
    "Perform the following actions:\n",
    "\n",
    "1. Summarize what the code is intended to do given the first test.\n",
    "2. List the names of the functions that this test is covering.\n",
    "3. Provide the code to make this test pass.\n",
    "4. Output the above steps in a JSON object in the following format: \n",
    "{{\n",
    "    \"test_summary\": \"1\", \n",
    "    \"function_names\": \"2\",\n",
    "    \"code\": \"3\"}}\n",
    "\n",
    "tests:\n",
    "```{unittest_code}```\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"test_summary\": \"The code is intended to test the addition function with positive integers.\",\n",
      "    \"function_names\": \"add\",\n",
      "    \"code\": \"def add(a, b):\\n    return a + b\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt)\n",
    "json_response = json.loads(response)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addition\n"
     ]
    }
   ],
   "source": [
    "# output response string to a python file. Need to keep track of how many files we've created\n",
    "\n",
    "filename = unittest_name.strip(\"_test\")\n",
    "print(filename)\n",
    "with open(f\"{filename}.py\", \"w\") as file:\n",
    "    file.write(json_response[\"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..F...\n",
      "======================================================================\n",
      "FAIL: test_add_non_numbers (addition_test.TestAddFunction.test_add_non_numbers)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jtmcgoffin/Documents/hackathon-2-22-24/Hackathon-2-22-24/projects/swarm-ai/addition_test.py\", line 23, in test_add_non_numbers\n",
      "    with self.assertRaises(TypeError):\n",
      "AssertionError: TypeError not raised\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.001s\n",
      "\n",
      "FAILED (failures=1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run tests against the response\n",
    "command = [\"python3\", \"-m\", \"unittest\", unittest_name]\n",
    "output = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "print(output.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n",
      "CompletedProcess(args=['python3', '-m', 'unittest', 'addition_test'], returncode=0, stdout='', stderr='......\\n----------------------------------------------------------------------\\nRan 6 tests in 0.000s\\n\\nOK\\n')\n"
     ]
    }
   ],
   "source": [
    "# iterate until all tests pass\n",
    "def get_success(test_output):\n",
    "    return \"OK\" in test_output\n",
    "\n",
    "def get_iteration_prompt(test_feedback):\n",
    "    return f\"\"\"\n",
    "\n",
    "    Some or all of the tests did not pass. The feedback from the test runner is in the triple backticks. Using that feedback, iterate on the code you previously provided to make the failing tests pass while maintaining that the passing tests continue to pass. Make sure that any non-test errors are addressed first and remove any unused imports. Perform the following actions:\n",
    "\n",
    "    1. Summarize what the code is intended to do given the tests.\n",
    "    2. List the names of the functions that these tests are covering.\n",
    "    3. Provide the code.\n",
    "    4. 4. Output the above steps in a JSON object in the following format: \n",
    "    {{\n",
    "    \"test_summary\": \"1\", \n",
    "    \"function_names\": \"2\",\n",
    "    \"code\": \"3\"}}\n",
    "\n",
    "    test feedback:\n",
    "    ```{test_feedback}```\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "num_iterations = 0\n",
    "\n",
    "while(not get_success(output.stderr)):\n",
    "    num_iterations += 1\n",
    "    print(f\"iteration {num_iterations}\")\n",
    "    response = get_completion(get_iteration_prompt(output.stderr))\n",
    "    json_response = json.loads(response)\n",
    "\n",
    "    # output response string to a python file. Need to keep track of how many files we've created\n",
    "    with open(f\"{filename}.py\", \"w\") as file:\n",
    "        file.write(json_response[\"code\"])\n",
    "\n",
    "    # run tests against the response\n",
    "    command = [\"python3\", \"-m\", \"unittest\", unittest_name]\n",
    "    output = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
